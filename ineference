import torch
import torch.nn as nn
import numpy as np
import pickle
from sklearn.metrics import accuracy_score, precision_score, f1_score
from Dataloader import ProteomicsDataset

# ====== 模型定义必须一致 ======
class SelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads=8):
        super(SelfAttention, self).__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.layer_norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        attn_output, _ = self.attention(x, x, x)
        return self.layer_norm(attn_output + x)

class SimpleNNWithAttention(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, reduced_dim=2048, embed_dim=2048, num_heads=4):
        super(SimpleNNWithAttention, self).__init__()
        self.input_projection = nn.Linear(input_dim, reduced_dim)
        self.self_attention = SelfAttention(embed_dim, num_heads)
        self.fc1 = nn.Linear(embed_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.3)

    def forward(self, x):
        x = self.input_projection(x)
        x = x.unsqueeze(1)
        x = self.self_attention(x)
        x = x.squeeze(1)
        x = self.dropout(x)
        x = self.relu(self.fc1(x))
        return self.fc2(x)


def compute_gradient_importance(model, X, y, criterion, device):
    X = X.to(device)
    y = y.to(device)
    X.requires_grad_()
    model.eval()
    outputs = model(X)
    loss = criterion(outputs, y)
    model.zero_grad()
    loss.backward()
    return X.grad.abs().mean(dim=0).detach().cpu().numpy()


def evaluate_model(model, data_loader, device):
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for data, labels in data_loader:
            data, labels = data.to(device), labels.to(device)
            outputs = model(data)
            preds = torch.argmax(outputs, dim=1)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())
    acc = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='binary')
    f1 = f1_score(y_true, y_pred, average='binary')
    return acc, precision, f1



def main():
    model_path = '/home/lixiaoyang/hello/model_fold2_run1.pt'
    dataset_path = '/home/lixiaoyang/hello/data/data_all.csv'
    dims_path = 'protein_dimensions.pkl'
    protein_names = ['RAB6A']

    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')

    dataset = ProteomicsDataset(dataset_path)
    input_dim = len(dataset.get_protein_dimensions()) - 1
    labels = np.array([lbl for _, lbl in dataset])
    protein_dims = pickle.load(open(dims_path, 'rb'))

    prot_idx = {name: protein_dims[name] for name in protein_names if name in protein_dims}

    test_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)
    model = SimpleNNWithAttention(input_dim, hidden_dim=1024, output_dim=2).to(device)
    model.load_state_dict(torch.load(model_path))
    model.eval()

    acc, precision, f1 = evaluate_model(model, test_loader, device)
    print(f"\n📊 Evaluation Metrics:")
    print(f"   Accuracy : {acc:.4f}")
    print(f"   Precision: {precision:.4f}")
    print(f"   F1 Score : {f1:.4f}")

    # 计算梯度重要性并打印指定蛋白排名
    criterion = nn.CrossEntropyLoss()
    X_batch, y_batch = next(iter(test_loader))
    grads = compute_gradient_importance(model, X_batch, y_batch, criterion, device)
    sorted_idx = np.argsort(grads)[::-1]

    print("\n🔬 Protein Importance Ranking:")
    for pname, pidx in prot_idx.items():
        if pidx >= len(grads):
            print(f"   {pname}: index out of range in gradient vector.")
            continue
        grad_val = grads[pidx]
        rank = int(np.where(sorted_idx == pidx)[0][0]) + 1
        print(f"   {pname} -> Gradient: {grad_val:.6f}, Rank: {rank}")


if __name__ == "__main__":
    main()
